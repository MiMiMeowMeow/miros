*trace dead*
= Observe =
  *What defines Success?*
  The trace would always run despite how long the program runs

  *What is the problem?*
  The trace instrumentation dies after the program runs for a long time.
  
  *What was the problem?*
  * The trace queue is set to keep a certain trace instrumentation memory.
  * The trace queue is a ring buffer so that the instrumentation doesn't become a
    memory leak
  * A trace event occurs upon each RTC.
  * A trace is intended to only write upon state transitions.
  * The previous design looked at the buffer length to determine if a trace
    occured.
  * Because the trace buffer is a ring buffer, it could be filled and still be
    full of good trace information.
  * By using the buffer size to determine if an event was new, the code was
    guarenteed to break once the ring buffer was filled.
  * Noticed upon testing that duplicate readings were printed with the same
    datetime information.
  * Changed the broken code to use time stamps instead of the ring buffer size
    appears to have fixed the problem.
  * The regression tests are passing... going to wrap this investagation up and
    cut another release

  *Evidence:*
    * When the TRC_RING_BUFFER_SIZE size was reduce from 150 to 50 the trace
      death occured at ~50 seconds.  This is roughly 1/3 of the time of 2:24
      seen for the trace death for a TRC_RING_BUFFER_SIZE size of 150.
      
    * The SPY_RING_BUFFER_SIZE is set to 500 and the TRC_RING_BUFFER_SIZE is
      smaller.
      
    * When the TRC_RING_BUFFER_SIZE was set to 5, the trace death occured
      after 3 trace outputs of the networked_horse_archer.py with time
      compression set to 50:1
      
    * At different time compressions the trace death occurs at the same number
      of listings
      
    * When changing the print_trace_after_rtc_if_live function:

          if(self.instrumented and self.live_trace and
             len(self.full.trace) != self.last_live_trace_size):
             
          to
          
          if(self.instrumented and self.live_trace):
          
      we passed the trace death, but duplicate trace items, one per spy event
      we printed to the trace output.  All of these events contained the same
      time stamp.
  
= Orient =
  *Symptoms:*
  Using the networked_horse_archer.py set to time compression 50:1 the trace
  dies after running for 2:24.  Turned on both trace and spy instrumentation.
  The spy instrumentation works even when the trace output stops.
  
  *Questions for trace dead:*
    Can you simplify your test?
    Can you baseline?
    Do you have enough information?
    What recently changed?
    Can you speed up the test?
    Why does this effect the trace and not the spy?

  *Assumptions:*
    The program is running past the trace death
    The spy instrumentation works forever

= Decide/Oracle Machining =
  *Idea/Hypotheses for trace dead: >=5*
  * [X] It smells like a queue problem
  * [X] Can you speed up the failure by shrinking the queue size?
  * [X] The trace buffer determines the number of trace output events that will
        occur prior to trace death
  * [X] It is related to the trace ring buffer
  * [X] It is something you haven't thought of yet
  * [X] Remind yourself how the trace buffer is used again
  * [X] It's not the buffer it is how it is dispatched
  * [X] The whole last_live_trace_size was a design mistake, use a time stamp
        instead

  *Chosen Idea/Hypothesis*
  The whole last_live_trace_size was a design mistake, use a time stamp instead
  
  *Plan to Expand-on-Idea/Disprove-Hypothesis*
  * Replace last_live_trace_size with last_live_trace_datetime
  * Adjust all trace print/callback code with blocks which will detect if the
    same trace output is being spat out each time using this new
    last_live_trace_datetime
  * Adjust how code is initialized so that the last_live_trace_datetime is set
    to 'now' on startup
  * Run the code and confirm that it doesn't address the issue, that the trace
    death will still occur.

= Act =

